<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <meta name="viewport" content="width=device-width">
    <meta name="google-site-verification" content="iplyfI0Y6pOe8NsNYwwrKe-QfSYamSeEhQiDFh93NiU">
    <title>Try Kubernetes with Vagrant - chris-rock
    </title>
    <link rel="alternate" href="http://lollyrock.com/feed.xml" type="application/rss+xml" title="fun &amp; sun">
    <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic|Anonymous+Pro:400,700,400italic,700italic|Merriweather:400,700,300">
    <link rel="stylesheet" href="/css/main.css">
    <script type="text/javascript">
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
      ga('create', 'UA-50992958-1', 'lollyrock.com');
      ga('send', 'pageview');
    </script>
    <script type="text/javascript">
      !function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');
      
    </script>
  </head>
  <body class="article-detail">
    <header class="header">
      <div class="content-wrap">
        <h1>Try Kubernetes with Vagrant</h1>
        <p class="author">Written by <span class="author"><a href="mailto:chris@lollyrock.com">Christoph Hartmann</a></span>
        </p>
      </div>
    </header>
    <div id="content">
      <div class="content-wrap">
        <article class="article">
          <section class="content"><p>To get familiar with kubernetes, it is always good to start with an example. This blog post will setup nginx running on&nbsp;kubernetes.</p>
<h2 id="prerequisites-aka-setup-the-cluster">Prerequisites aka setup the&nbsp;cluster</h2>
<p>Before we are able to start, we need to download kubernetes and install the command line. To prepare the&nbsp;setup:</p>
<ul>
<li>install <a href="http://www.vagrantup.com/downloads.html">Vagrant</a>&nbsp;(&gt;1.6.2)</li>
<li>install <a href="https://www.virtualbox.org/wiki/Downloads">VirtualBox</a></li>
</ul>
<p>Then clone the git repository and set the provider for kubernetes setup&nbsp;scripts </p>
<pre><code class="lang-bash">git clone https://github.com/GoogleCloudPlatform/kubernetes.git
<span class="keyword">export</span> KUBERNETES_PROVIDER=vagrant
<span class="built_in">cd</span> kubernetes
</code></pre>
<p>Now, we are ready to start the Vagrant cluster. With <code>kube-up.sh</code> vagrant will provision each machine in the cluster. All necessary components to run kubernetes are installed automatically. By default, each <span class="caps">VM</span> is running Fedora as base&nbsp;OS.</p>
<pre><code class="lang-bash">$ ./cluster/kube-up.sh
Starting cluster using provider: vagrant
... calling verify-prereqs
... calling kube-up

...

Wrote config <span class="keyword">for</span> vagrant to /Users/chris/.kube/config
Each machine instance has been created/updated.
  Now waiting <span class="keyword">for</span> the Salt provisioning process to complete on each machine.
  This can take some time based on your network, disk, and cpu speed.
  It is possible <span class="keyword">for</span> an error to occur during Salt provision of cluster and this could loop forever.
Validating master
Validating minion-<span class="number">1</span>
.....
Waiting <span class="keyword">for</span> each minion to be registered with cloud provider

...

Kubernetes cluster is running.  The master is running at:

  https://<span class="number">10.245</span>.<span class="number">1.2</span>

The user name and password to use is located <span class="keyword">in</span> ~/.kubernetes_vagrant_auth.

... calling validate-cluster
Found <span class="number">1</span> nodes.
     <span class="number">1</span>  <span class="caps">NAME</span>         LABELS    STATUS
     <span class="number">2</span>  <span class="number">10.245</span>.<span class="number">1.3</span>   &lt;none&gt;    Ready
Validate output:
<span class="caps">NAME</span>                 STATUS    MESSAGE   ERROR
controller-manager   Healthy   ok        nil
scheduler            Healthy   ok        nil
etcd-<span class="number">0</span>               Healthy   {<span class="string">"action"</span>:<span class="string">"get"</span>,<span class="string">"node"</span>:{<span class="string">"dir"</span>:<span class="literal">true</span>,<span class="string">"nodes"</span>:[{<span class="string">"key"</span>:<span class="string">"/registry"</span>,<span class="string">"dir"</span>:<span class="literal">true</span>,<span class="string">"modifiedIndex"</span>:<span class="number">3</span>,<span class="string">"createdIndex"</span>:<span class="number">3</span>}]}}
                     nil
Cluster validation succeeded
Done, listing cluster services:

Kubernetes master is running at https://<span class="number">10.245</span>.<span class="number">1.2</span>
kube-dns is running at https://<span class="number">10.245</span>.<span class="number">1.2</span>/api/v1beta3/proxy/namespaces/default/services/kube-dns
</code></pre>
<p>Verify that kubernetes is&nbsp;running:</p>
<pre><code class="lang-bash">$ curl --user vagrant:vagrant --insecure https://<span class="number">10.245</span>.<span class="number">1.2</span>/ 
{
  <span class="string">"paths"</span>: [
    <span class="string">"/api"</span>,
    <span class="string">"/api/v1beta1"</span>,
    <span class="string">"/api/v1beta2"</span>,
    <span class="string">"/api/v1beta3"</span>,
    <span class="string">"/healthz"</span>,
    <span class="string">"/healthz/ping"</span>,
    <span class="string">"/logs/"</span>,
    <span class="string">"/metrics"</span>,
    <span class="string">"/static/"</span>,
    <span class="string">"/swagger-ui/"</span>,
    <span class="string">"/swaggerapi/"</span>,
    <span class="string">"/validate"</span>,
    <span class="string">"/version"</span>
  ]
}
</code></pre>
<p>You could also be able to ssh into master and&nbsp;minion:</p>
<pre><code class="lang-bash">$ vagrant ssh master
$ vagrant ssh minion-<span class="number">1</span>
</code></pre>
<p>Further details are available at <a href="https://github.com/GoogleCloudPlatform/kubernetes/blob/master/docs/getting-started-guides/vagrant.md">Kubernetes Vagrant&nbsp;Documentation</a></p>
<h2 id="do-some-magic-with-kubernetes-cli">Do some magic with Kubernetes&nbsp;<span class="caps">CLI</span></h2>
<p>You could use the <code>cluster/kubectl.sh</code>, but I find it much easier to install the <code>kubectl</code> binary. This can be easily done&nbsp;via:</p>
<pre><code class="lang-bash"><span class="comment"># use mac binary</span>
$ wget https://storage.googleapis.com/kubernetes-release/release/v0.<span class="number">17.0</span>/bin/darwin/amd64/kubectl -O /usr/local/bin/kubectl
$ chmod +x /usr/local/bin/kubectl
$ kubectl version
</code></pre>
<p>Now, we have the <code>kubectl</code> command ready for&nbsp;use.</p>
<h2 id="start-nginx-cluster-without-pod-definition">Start Nginx cluster without pod&nbsp;definition</h2>
<p>This first example uses the docker nginx image and runs it as a cluster. It uses a replica of 3 during bootstrap. Afterwards we change the replica to 2 containers. Finally we destroy all running&nbsp;containers.</p>
<pre><code class="lang-bash"><span class="comment"># Create new nginx cluster</span>
$ kubectl run-container webserver --image=nginx --replicas=<span class="number">3</span> --port=<span class="number">80</span>
<span class="caps">CONTROLLER</span>   CONTAINER(S)   IMAGE(S)   SELECTOR                  REPLICAS
webserver    webserver      nginx      run-container=webserver   <span class="number">3</span>

<span class="comment"># Show all running pods</span>
$ kubectl get pods <span class="operator">-l</span> run-container=webserver
<span class="caps">POD</span>               IP            CONTAINER(S)   IMAGE(S)   HOST                    LABELS                    STATUS    CREATED      MESSAGE
webserver-<span class="number">6</span>de4a   <span class="number">10.246</span>.<span class="number">1.17</span>                             <span class="number">10.245</span>.<span class="number">1.3</span>/<span class="number">10.245</span>.<span class="number">1.3</span>   run-container=webserver   Running   <span class="number">27</span> seconds   
                                webserver      nginx                                                        Running   <span class="number">25</span> seconds   
webserver-koxmk   <span class="number">10.246</span>.<span class="number">1.19</span>                             <span class="number">10.245</span>.<span class="number">1.3</span>/<span class="number">10.245</span>.<span class="number">1.3</span>   run-container=webserver   Running   <span class="number">27</span> seconds   
                                webserver      nginx                                                        Running   <span class="number">24</span> seconds   
webserver<span class="operator">-lt</span>0e9   <span class="number">10.246</span>.<span class="number">1.18</span>                             <span class="number">10.245</span>.<span class="number">1.3</span>/<span class="number">10.245</span>.<span class="number">1.3</span>   run-container=webserver   Running   <span class="number">27</span> seconds   
                                webserver      nginx                                                        Running   <span class="number">24</span> seconds   

$ kubectl get replicationControllers <span class="operator">-l</span> run-container=webserver
<span class="caps">CONTROLLER</span>   CONTAINER(S)   IMAGE(S)   SELECTOR                  REPLICAS
webserver    webserver      nginx      run-container=webserver   <span class="number">3</span>

<span class="comment"># Resize the replica:</span>
$ kubectl resize rc webserver --replicas=<span class="number">2</span>
resized

$ kubectl get replicationControllers <span class="operator">-l</span> run-container=webserver
<span class="caps">CONTROLLER</span>   CONTAINER(S)   IMAGE(S)   SELECTOR                  REPLICAS
webserver    webserver      nginx      run-container=webserver   <span class="number">2</span>

<span class="comment"># Show all kube nodes, previously `kubectl get minions`</span>
$ kubectl get nodes
<span class="caps">NAME</span>         LABELS    STATUS
<span class="number">10.245</span>.<span class="number">1.3</span>   &lt;none&gt;    Ready

<span class="comment"># Delete running nginx pod with all replicas and services</span>
$ kubectl stop pods,services <span class="operator">-l</span> run-container=webserver
$ kubectl delete pods,services <span class="operator">-l</span> run-container=webserver
$ kubectl resize rc webserver --replicas=<span class="number">2</span>
pods/webserver-<span class="number">6</span>de4a
pods/webserver-koxmk
</code></pre>
<h2 id="start-nginx-cluster-with-pods-definition">Start Nginx cluster with pods&nbsp;definition</h2>
<p>We define a pod definition <code>01-nginx.yaml</code> to get nginx up and running. The meta data describes the name and labels of the container. This is required to define selectors for replicas, as we use it later. The <code>containers</code> define the used docker image. Create the following&nbsp;file:</p>
<pre><code class="lang-yaml"># 01-nginx.yaml
apiVersion: v1beta3
kind: Pod
metadata:
  name: www
  labels: 
    name: www-nginx
spec:
  containers:
    - name: nginx
      image: nginx
      ports:
        - containerPort: 80
</code></pre>
<p>Load the pod into&nbsp;kubernetes.</p>
<pre><code class="lang-bash"><span class="comment"># Start nginx pod/container</span>
$ kubectl create <span class="operator">-f</span> <span class="number">01</span>-nginx.yaml 
pods/www

<span class="comment"># Check that nginx is running</span>
$ kubectl get pods <span class="operator">-l</span> name=www-nginx
</code></pre>
<p>Now, we adapt the replica by creating a new <code>ReplicationController</code> definition:</p>
<pre><code class="lang-yaml"># 01-nginx-replica.yaml
apiVersion: v1beta3
kind: ReplicationController
metadata:
  name: nginx-controller
spec:
  replicas: 2
  # selector identifies the set of Pods that this
  # replication controller is responsible for managing
  selector:
    name: www-nginx
  # podTemplate defines the &#39;cookie cutter&#39; used for creating
  # new pods when necessary
  template:
    metadata:
      name: www
      labels:
        # Important: these labels need to match the selector above
        # The api server enforces this constraint.
        name: www-nginx
    spec:
      containers:
        - name: nginx
          image: nginx
          ports:
            - containerPort: 80
</code></pre>
<p>Load the new definition into&nbsp;kubernetes:</p>
<pre><code class="lang-bash"><span class="comment"># Increase replica of the current nginx instances</span>
$ kubectl create <span class="operator">-f</span> <span class="number">01</span>-nginx-replica.yaml
replicationcontrollers/nginx-controller

<span class="comment"># Check the replica</span>
$ kubectl get replicationControllers <span class="operator">-l</span> name=www-nginx         
<span class="caps">CONTROLLER</span>         CONTAINER(S)   IMAGE(S)   SELECTOR         REPLICAS
nginx-controller   nginx          nginx      name=www-nginx   <span class="number">2</span>

<span class="comment"># Verify the connection to the pods</span>
$ kubectl get pods <span class="operator">-l</span> name=www-nginx                                                                                ✘<span class="number">130</span> 
<span class="caps">POD</span>                      IP            CONTAINER(S)   IMAGE(S)   HOST                    LABELS           STATUS    CREATED      MESSAGE
nginx-controller-lfeo7   <span class="number">10.246</span>.<span class="number">1.15</span>                             <span class="number">10.245</span>.<span class="number">1.3</span>/<span class="number">10.245</span>.<span class="number">1.3</span>   name=www-nginx   Running   <span class="number">21</span> minutes   
                                       nginx          nginx                                               Running   <span class="number">21</span> minutes   
www                      <span class="number">10.246</span>.<span class="number">1.11</span>                             <span class="number">10.245</span>.<span class="number">1.3</span>/<span class="number">10.245</span>.<span class="number">1.3</span>   name=www-nginx   Running   <span class="number">28</span> minutes   
                                       nginx          nginx
</code></pre>
<p>You could easily verify that multiple nginx instances are&nbsp;running:</p>
<pre><code class="lang-bash">$ vagrant ssh minion-<span class="number">1</span>
[vagrant@kubernetes-minion-<span class="number">1</span> ~]$ curl http://<span class="number">10.246</span>.<span class="number">1.15</span>
[vagrant@kubernetes-minion-<span class="number">1</span> ~]$ curl http://<span class="number">10.246</span>.<span class="number">1.11</span>
</code></pre>
<p>The different containers are not useful by itself. We would like to abstract all instances by a service. Create and load the service definition into&nbsp;kubernetes.</p>
<pre><code class="lang-yaml"># 01-nginx-service.yaml
apiVersion: v1beta3
kind: Service
metadata:
  name: nginx-endpoint
  labels: 
    name: www-nginx
spec:
  ports:
    - port: 8000 # the port that this service should serve on
      # the container on each pod to connect to, can be a name
      # (e.g. &#39;www&#39;) or a number (e.g. 80)
      targetPort: 80
      protocol: TCP
  # public ip of minion, only required for vagrant
  # just like the selector in the replication controller,
  # but this time it identifies the set of pods to load balance
  # traffic to.
  selector:
    name: nginx
</code></pre>
<pre><code class="lang-bash"><span class="comment"># Add public endpoint</span>
$ kubectl create <span class="operator">-f</span> <span class="number">01</span>-nginx-service.yaml
services/nginx-endpoint

<span class="comment"># Show all running endpoints</span>
$ kubectl get services <span class="operator">-l</span> name=www-nginx 
<span class="caps">NAME</span>             LABELS           SELECTOR     IP(S)           PORT(S)
nginx-endpoint   name=www-nginx   name=nginx   <span class="number">10.247</span>.<span class="number">212.62</span>   <span class="number">8000</span>/<span class="caps">TCP</span>
</code></pre>
<p>Now we set up a <span class="caps">HTTP</span> Health Checks that will restart a the nginx container in case the application&nbsp;crashes.</p>
<pre><code class="lang-yaml">apiVersion: v1beta3
kind: Pod
metadata:
  name: nginx-healthcheck
  labels: 
    name: www-nginx
spec:
  containers:
    - name: nginx
      image: nginx
      # defines the health checking
      livenessProbe:
        # an http probe
        httpGet:
          path: /_status/healthz
          port: 80
        # length of time to wait for a pod to initialize
        # after pod startup, before applying health checking
        initialDelaySeconds: 30
        timeoutSeconds: 1
      ports:
        - containerPort: 80
</code></pre>
<p>Add the health check to your&nbsp;cluster</p>
<pre><code class="lang-bash"><span class="comment"># add a health check</span>
kubectl create <span class="operator">-f</span> <span class="number">01</span>-nginx-health-check.yaml
</code></pre>
<p>Everything is setup properly. To destory the cluster, delete all definitions from&nbsp;kubernetes:</p>
<pre><code># Delete all added parts
$ kubectl delete -f 01-nginx-service.yaml 
$ kubectl delete -f 01-nginx.yaml
$ kubectl delete -f 01-nginx-health-check.yaml
</code></pre><p>Have fun with&nbsp;containers!</p>
<p>If you have any questions contact me via <a href="https://twitter.com/chri_hartmann">Twitter @chri_hartmann</a> or <a href="https://github.com/chris-rock">Github</a></p>
</section>
        </article>
      </div>
    </div>
    <footer>
      <div class="content-wrap">
        <div class="social center">
          <div class="network"><a href="javascript:window.location=%22http://news.ycombinator.com/submitlink?u=%22+encodeURIComponent(document.location)+%22&amp;amp;t=%22+encodeURIComponent(document.title)" class="hn-link">Discuss on Hacker News</a></div>
          <div class="network twitter"><a https://twitter.com/share data-via="chri_hartmann" data-count="none" class="twitter-share-button">Tweet</a></div>
        </div>
        <div class="nav"><a href="/">« Full blog</a></div>
        <section class="about">
        </section>
        <section class="copy">
          <p>&copy; 2015 Christoph Hartmann &mdash; powered by&nbsp;<a href="https://github.com/jnordberg/wintersmith">Wintersmith</a>
          </p>
        </section>
      </div>
    </footer>
  </body>
</html>